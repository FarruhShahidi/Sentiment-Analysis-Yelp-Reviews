{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import process_tweet, lookup\n",
    "import random\n",
    "from nltk.corpus import stopwords, twitter_samples\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import string\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>My wife took me here on my birthday for breakf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>I have no idea why some people give bad review...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>Rosie, Dakota, and I LOVE Chaparral Dog Park!!...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>General Manager Scott Petello is a good egg!!!...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5</td>\n",
       "      <td>Drop what you're doing and drive here. After I...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   stars                                               text\n",
       "0      5  My wife took me here on my birthday for breakf...\n",
       "1      5  I have no idea why some people give bad review...\n",
       "3      5  Rosie, Dakota, and I LOVE Chaparral Dog Park!!...\n",
       "4      5  General Manager Scott Petello is a good egg!!!...\n",
       "6      5  Drop what you're doing and drive here. After I..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('yelp.csv')\n",
    "# select the columns that we need\n",
    "df = df[['stars', 'text' ]]\n",
    "# work with only 1 and 5 stars\n",
    "df = df[(df.stars==1) | (df.stars==5)]\n",
    "df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3670 416\n"
     ]
    }
   ],
   "source": [
    "# Split the reviews into positive(5 star) and negative (1 star)\n",
    "# use 80% for train and 20% for validation\n",
    "\n",
    "\n",
    "test_pos = []\n",
    "train_pos = []\n",
    "test_neg = []\n",
    "train_neg = []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    r = random.randint(0,99)\n",
    "    if  r < 90:\n",
    "        if row[0] == 5:\n",
    "            train_pos.append(row[1])\n",
    "        else:\n",
    "            train_neg.append(row[1])\n",
    "    else:\n",
    "        if row[0] == 5:\n",
    "            test_pos.append(row[1])\n",
    "        else:\n",
    "            test_neg.append(row[1])\n",
    "\n",
    "\n",
    "\n",
    "train_x = train_pos + train_neg\n",
    "test_x = test_pos + test_neg\n",
    "print(len(train_x), len(test_x))\n",
    "# avoid assumptions about the length of all_positive_tweets\n",
    "train_y = np.append(np.ones(len(train_pos)), np.zeros(len(train_neg)))\n",
    "test_y = np.append(np.ones(len(test_pos)), np.zeros(len(test_neg)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_reviews(result, reviews, ys):\n",
    "    '''\n",
    "    Input:\n",
    "        result: a dictionary that will be used to map each pair to its frequency\n",
    "        tweets: a list of reviews\n",
    "        ys: a list corresponding to the sentiment of each review (either 0 or 1)\n",
    "    Output:\n",
    "        result: a dictionary mapping each pair to its frequency\n",
    "    '''\n",
    "\n",
    "    \n",
    "    for y, review in zip(ys, reviews):\n",
    "        for word in process_tweet(review):\n",
    "            # define the key, which is the word and label tuple\n",
    "            pair = (word, y)\n",
    "\n",
    "            # if the key exists in the dictionary, increment the count\n",
    "            if pair in result:\n",
    "                result[pair] += 1\n",
    "\n",
    "            # else, if the key is new, add it to the dictionary and set the count to 1\n",
    "            else:\n",
    "                result[pair] = 1\n",
    "   \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('happi', 1): 1, ('trick', 0): 1, ('sad', 0): 1, ('tire', 0): 2}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing the function\n",
    "\n",
    "\n",
    "result = {}\n",
    "reviews = ['i am happy', 'i am tricked', 'i am sad', 'i am tired', 'i am tired']\n",
    "ys = [1, 0, 0, 0, 0]\n",
    "count_reviews(result, reviews, ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train your model using Naive Bayes\n",
    "\n",
    "Naive bayes is an algorithm that could be used for sentiment analysis. It takes a short time to train and also has a short prediction time.\n",
    "\n",
    "#### So how do you train a Naive Bayes classifier?\n",
    "- The first part of training a naive bayes classifier is to identify the number of classes that you have.\n",
    "- You will create a probability for each class.\n",
    "$P(D_{pos})$ is the probability that the document is positive.\n",
    "$P(D_{neg})$ is the probability that the document is negative.\n",
    "Use the formulas as follows and store the values in a dictionary:\n",
    "\n",
    "$$P(D_{pos}) = \\frac{D_{pos}}{D}\\tag{1}$$\n",
    "\n",
    "$$P(D_{neg}) = \\frac{D_{neg}}{D}\\tag{2}$$\n",
    "\n",
    "Where $D$ is the total number of documents, or reviews in this case, $D_{pos}$ is the total number of positive reviews and $D_{neg}$ is the total number of negative reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prior and Logprior\n",
    "\n",
    "The prior probability represents the underlying probability in the target population that a review is positive versus negative.  In other words, if we had no specific information and blindly picked a review out of the population set, what is the probability that it will be positive versus that it will be negative? That is the \"prior\".\n",
    "\n",
    "The prior is the ratio of the probabilities $\\frac{P(D_{pos})}{P(D_{neg})}$.\n",
    "We can take the log of the prior to rescale it, and we'll call this the logprior\n",
    "\n",
    "$$\\text{logprior} = log \\left( \\frac{P(D_{pos})}{P(D_{neg})} \\right) = log \\left( \\frac{D_{pos}}{D_{neg}} \\right)$$.\n",
    "\n",
    "Note that $log(\\frac{A}{B})$ is the same as $log(A) - log(B)$.  So the logprior can also be calculated as the difference between two logs:\n",
    "\n",
    "$$\\text{logprior} = \\log (P(D_{pos})) - \\log (P(D_{neg})) = \\log (D_{pos}) - \\log (D_{neg})\\tag{3}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Positive and Negative Probability of a Word\n",
    "To compute the positive probability and the negative probability for a specific word in the vocabulary, we'll use the following inputs:\n",
    "\n",
    "- $freq_{pos}$ and $freq_{neg}$ are the frequencies of that specific word in the positive or negative class. In other words, the positive frequency of a word is the number of times the word is counted with the label of 1.\n",
    "- $N_{pos}$ and $N_{neg}$ are the total number of positive and negative words for all documents (for all tweets), respectively.\n",
    "- $V$ is the number of unique words in the entire set of documents, for all classes, whether positive or negative.\n",
    "\n",
    "We'll use these to compute the positive and negative probability for a specific word using this formula:\n",
    "\n",
    "$$ P(W_{pos}) = \\frac{freq_{pos} + 1}{N_{pos} + V}\\tag{4} $$\n",
    "$$ P(W_{neg}) = \\frac{freq_{neg} + 1}{N_{neg} + V}\\tag{5} $$\n",
    "\n",
    "Notice that we add the \"+1\" in the numerator for additive smoothing.  This [wiki article](https://en.wikipedia.org/wiki/Additive_smoothing) explains more about additive smoothing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Log likelihood\n",
    "To compute the loglikelihood of that very same word, we can implement the following equations:\n",
    "\n",
    "$$\\text{loglikelihood} = \\log \\left(\\frac{P(W_{pos})}{P(W_{neg})} \\right)\\tag{6}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create `freqs` dictionary\n",
    "- Given your `count_reviews()` function, you can compute a dictionary called `freqs` that contains all the frequencies.\n",
    "- In this `freqs` dictionary, the key is the tuple (word, label)\n",
    "- The value is the number of times it has appeared.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqs = count_reviews({}, train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_naive_bayes(freqs, train_x, train_y):\n",
    "    '''\n",
    "    Input:\n",
    "        freqs: dictionary from (word, label) to how often the word appears\n",
    "        train_x: a list of reviews\n",
    "        train_y: a list of labels correponding to the review (0,1)\n",
    "    Output:\n",
    "        logprior: the log prior. (equation 3 above)\n",
    "        loglikelihood: the log likelihood of you Naive Bayes equation. (equation 6 above)\n",
    "    '''\n",
    "    loglikelihood = {}\n",
    "    logprior = 0\n",
    "\n",
    "\n",
    "    # calculate V, the number of unique words in the vocabulary\n",
    "    vocab = set([pair[0] for pair in freqs.keys()])\n",
    "    V = len(vocab)\n",
    "\n",
    "    # calculate N_pos and N_neg\n",
    "    N_pos = N_neg = 0\n",
    "    for pair in freqs.keys():\n",
    "        # if the label is positive (greater than zero)\n",
    "        if pair[1] > 0:\n",
    "\n",
    "            # Increment the number of positive words by the count for this (word, label) pair\n",
    "            N_pos += freqs[pair]\n",
    "\n",
    "        # else, the label is negative\n",
    "        else:\n",
    "\n",
    "            # increment the number of negative words by the count for this (word,label) pair\n",
    "            N_neg += freqs[pair]\n",
    "\n",
    "    # Calculate D, the number of documents\n",
    "    D = len(train_y)\n",
    "\n",
    "    # Calculate D_pos, the number of positive documents \n",
    "    D_pos = np.sum(train_y)\n",
    "\n",
    "    # Calculate D_neg, the number of negative documents \n",
    "    D_neg = D - D_pos\n",
    "\n",
    "    # Calculate logprior\n",
    "    logprior = np.log(D_pos/ D_neg)\n",
    "\n",
    "    # For each word in the vocabulary...\n",
    "    for word in vocab:\n",
    "        # get the positive and negative frequency of the word\n",
    "        freq_pos = lookup(freqs, word, 1)\n",
    "        freq_neg = lookup(freqs, word, 0)\n",
    "\n",
    "        # calculate the probability that each word is positive, and negative\n",
    "        p_w_pos = (freq_pos + 1) / (N_pos + V)\n",
    "        p_w_neg = (freq_neg + 1) / (N_neg + V)\n",
    "\n",
    "        # calculate the log likelihood of the word\n",
    "        loglikelihood[word] = np.log(p_w_pos/ p_w_neg)\n",
    "\n",
    "    \n",
    "\n",
    "    return logprior, loglikelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4845500511539722\n",
      "14466\n"
     ]
    }
   ],
   "source": [
    "logprior, loglikelihood = train_naive_bayes(freqs, train_x, train_y)\n",
    "print(logprior)\n",
    "print(len(loglikelihood))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Test  naive bayes\n",
    "\n",
    "$$ p = logprior + \\sum_i^N (loglikelihood_i)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C4 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "def naive_bayes_predict(review, logprior, loglikelihood):\n",
    "    '''\n",
    "    Input:\n",
    "        review: a string\n",
    "        logprior: a number\n",
    "        loglikelihood: a dictionary of words mapping to numbers\n",
    "    Output:\n",
    "        p: the sum of all the logliklihoods of each word in the review (if found in the dictionary) + logprior (a number)\n",
    "\n",
    "    '''\n",
    "    \n",
    "    # process the review to get a list of words\n",
    "    word_l = process_tweet(review)\n",
    "\n",
    "    # initialize probability to zero\n",
    "    p = 0\n",
    "\n",
    "    # add the logprior\n",
    "    p += logprior\n",
    "\n",
    "    for word in word_l:\n",
    "\n",
    "        # check if the word exists in the loglikelihood dictionary\n",
    "        if word in loglikelihood:\n",
    "            # add the log likelihood of that word to the probability\n",
    "            p += loglikelihood[word]\n",
    "\n",
    "    \n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The expected output is 1.8619296068156412\n"
     ]
    }
   ],
   "source": [
    "my_review = 'We smiled.'\n",
    "p = naive_bayes_predict(my_review, logprior, loglikelihood)\n",
    "print('The expected output is', p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_naive_bayes(test_x, test_y, logprior, loglikelihood):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        test_x: A list of reviews\n",
    "        test_y: the corresponding labels for the list of reviews\n",
    "        logprior: the logprior\n",
    "        loglikelihood: a dictionary with the loglikelihoods for each word\n",
    "    Output:\n",
    "        accuracy: (# of reviews classified correctly)/(total # of reviews)\n",
    "    \"\"\"\n",
    "    accuracy = 0  \n",
    "    y_hats = []\n",
    "    for review in test_x:\n",
    "        # if the prediction is > 0\n",
    "        if naive_bayes_predict(review, logprior, loglikelihood) > 0:\n",
    "            # the predicted class is 1\n",
    "            y_hat_i = 1\n",
    "        else:\n",
    "            # otherwise the predicted class is 0\n",
    "            y_hat_i = 0\n",
    "\n",
    "        # append the predicted class to the list y_hats\n",
    "        y_hats.append(y_hat_i)\n",
    "    # error is the average of the absolute values of the differences between y_hats and test_y\n",
    "    error = np.mean(np.absolute(y_hats - test_y))\n",
    "    # Accuracy is 1 minus the error\n",
    "    accuracy = 1- error\n",
    "\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes accuracy = 0.9399\n"
     ]
    }
   ],
   "source": [
    "print(\"Naive Bayes accuracy = %0.4f\" %\n",
    "      (test_naive_bayes(test_x, test_y, logprior, loglikelihood)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Error Analysis__\n",
    "\n",
    "Below we print some reviews which our algorithm incorrectly predicted. Why do you think the algorithm incorrecly labeled them?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Truth Predicted Review\n",
      "1\t0.00\tb'last week sister order steak cook medium came well done kind disappoint oh well last night fianc buy-one-get-one-fre coupon head order steak cook medium ask make point put order issu last week food came ask cut steak make sure cook way want bit medium-wel medium hungri look good gave go ahead start meal realli good steak waitress came doubl check steak cook correctli assur fine minut later manag came apolog steak done offer replac assur mayb littl overdon good happi meal nice show great custom servic though littl overzeal :) bill came made steak free one coupon even though mine expens two meal order great custom servic good food'\n",
      "1\t0.00\tb'great experi love carpet care repair friendli courteou enthusiast everi time spoke phone work incred hire clean carpet apart leas offic apt complex told carpet bad shape would trash quick asid live apart respons damag ... love carpet care good job clean carpet look nearli new truli outstand importantli price quot beforehand gave honest account condit size final price love cheap desper upset enough realli could charg anyth wonder honest hardwork help could never say enough good thing compani'\n",
      "1\t0.00\tb'dude .. manga ... card tabl . ... heck place sell comic outrag talk tri help comic clerk job give huge sigh exasper look ask perform surgeri help friendli realli bother guy weird need go back atom'\n",
      "1\t0.00\tb'bruce save day comput crash morn gave call abl come day went whole system offic even updat anti-viru free avg instead pay crappi norton servic abl go recov file found oper system corrupt save day complet everyth day thank bruce'\n",
      "1\t0.00\tb\"weekend debaucheri spent 18 somehow broke two lamp hang wall like high wall like head level ask rememb happen either tri steal lamp room maid kept catch us wrote u 2 lamp :( one lampshad left one night friend jonathan got realli drunk angri lone went pool threw poolsid furnitur pool serious destroy motel i'm kid much filth one room see carpet felt realli realli bad whoever clean best part get charg damag caus thank motel 6 wonder use point low ... use yelp often showcas anecdot actual review thing i'm useless\"\n",
      "1\t0.00\tb\"last time vig trick .. see glass door open back restaur nice sometim go door outsid take trip loo close like asshol attempt walk back outsid way came instead bash head knee glass window front 35 peopl mind hey least amaz wine special night sauci enough laugh take bow hid bocc ball pit remaind even somehow acquir 200 bruis love vig even i'm embarrass shit\"\n",
      "1\t0.00\tb'busi place everyon nice keep go cell lot late baggag origin destin charg lot deliv better get bag check 45 mn earli ...'\n",
      "1\t0.00\tb\"grew san antonio austin travel extens sampl bbq countri term texas-styl mean beef first period good smoke ain't mesquit oak king guy chang face phoenician bbq forev shame scottsdal locat touch ... well scottsdal friggin moron need go 24th e roeser tri hand best q area includ cave creek guy texa know what where how bar-b-qu meal carton butcher paper snob think declass pleas favor ... head austin better yet lockhart stand front line stubb black' kruez market loudli proclaim thought bbq aficionado let know head hang watch ensu festiv freud said best bbq horrif anally-retent unfulfil dream aunti shut grow ya infantil gomer ...\"\n",
      "0\t1.00\tb'place anymor'\n",
      "0\t1.00\tb'kind place serv chicken finger ranch dress sauc honey mustard cane secret sauc say eewww thought sauc tast terribl big fan honey mustard occas eat noth els even good coleslaw aw also say chicken finger juici also bland 2 item tri realli item menu texa toast fri think overal would never go back'\n",
      "0\t1.00\tb'like stuck scottsdal vibe good place food impress nice outdoor seat'\n",
      "0\t1.00\tb'place close good riddanc'\n",
      "0\t1.00\tb'offic move area drive around saw sign best sandwich ... um want come show cook chicken chees steak first wrap let sandwich sweat make bun soft chewi second put meat 50 bun toast crunchi price 12 chicken chees chip drink .. kid price hope best ever chang capriotti word get tough market'\n",
      "0\t1.00\tb'soggi flavorless pizza serv unwarm ceram plate carbonara wateri made cream instead egg gnocchi tough oppos light airi'\n",
      "0\t1.00\tb'buffet like never go particular one food look fresh went food line select favorit stack bowl sky select sauc turn grill cook dous dish water quickli cook ingredi return new bowl select crispi wonton bit rice soup add tray cost select rice old burnt pan chose hot sour soup favorit free-rang refil beverag sat eat impress could muster tri second spoon soup absolut disgust bowl ingredi bland perhap overus water grill almost certain tofu gone bad sliver meat husband select tast fowl well obvious get far meal leav alreadi paid would sever complaint chang select could taken left over home opt leav instead suffic say recommend'\n",
      "0\t1.00\tb\"good understand peopl give place even three star rare give place low rate big fat greek restaur noth gimmick anymor gone bad wors food even close represent good greek food tast like i'v place sever time year locat other use lower end ok ... never good get pa place howev went lunch day gotten point even go back quick bite order garlic hummu overli full garlic home made seem load garlic powder oppos fresh hummu roast garlic oliv order rough best ugh even think overli season fresh rice cook soft lack sort textur first rule good let alon great food fresh place need rescu less critic think overal represent happen place menu' total trash ratti tatter dirti lamin come dirti favor go elsewher\"\n",
      "0\t1.00\tb'write review optic depart accident poke eye branch yesterday knew damag went gilbert costco optician ask could see emerg basi receptionist ask could come back 5 hour state emerg could ask dr could see ask said time find unbeliev emerg basi turn someon away costco optic patient year recogn patient time said could wait could least take 5 minut tell go hospit dont ever wast time go need immedi attent emerg make room'\n",
      "0\t1.00\tb'close sunday look like lost sunday dollar busi monday saturday'\n",
      "0\t1.00\tb\"feel like step time machin land 1987 decor updat least 20 year look like fall apart tv mostli 80 style tv well although coupl flat screen well trash hang color pizza greasi like also chewi gross wing drown much sauc find might well get glass honey bbq sauc drink straight servic interest guy work wear capri pant higher voic girlfriend price bad think like larg pizza wing 2 soda i'd rather pay littl bit go upper crust pizza street\"\n",
      "0\t1.00\tb\"enter store visual overwhelm mani thing take thought i'd found new groceri store wander good 40 min check everyth ... ether wore began realiz someth select wide look old race fresh fish notic even pack ice littl fish eyebal cloudi sunken socket look frozen section found fish sell date 02/2010 fish pack store pack sell date can box food past expir date check place maricopa counti health depart fare share custom complaint driven inspect\"\n",
      "0\t1.00\tb\"place oiliest food i'v ever eaten life carri one even two dish entre soak oil rins water eggplant dish three time eat without feel sick stomach wish could'v wash dish one could washed--it wide-ric noodl dish definit tast msg dish realli sure fuss\"\n",
      "0\t1.00\tb\"guess east coast midwest thing realli get i'm picki eater hate wast food money could finish plate order french toast awar french toast go deep fri serious put fork piec fri toast hit eye husband also impress food someon would like suggest someth delici menu tri pleas let know current want vomit everi time see chompie'\"\n",
      "0\t1.00\tb\"shock place 4 star first alway tell mexican food place good base bean salsa well guess suck eat el molino' never come\"\n",
      "0\t1.00\tb\"birthday wife mom dear friend head fleming' steakhous chandler az heard great thing place want experi unforgett steak dish unfortun restaur love dim-lit adequ atmospher expens steak place seat immedi tend wait staff small roll bread two kind butter good insuffici four hungri soul polish right away one offer bring waiter told us daili special order food wine short wait four larg dinner plate arriv lucki one steak larg rib eye steak order medium-rar tast good despit two larg blob fat sit pretti middl larg dinner plate noth els plate steak look around wife' plate believ fillet mignon lay middl larg dinner plate without anyth garnish glaze leaf slice whatev price ask restaur felt like insult us poor present steak ask waiter side dish mash potato parmesan arriv believ two scraggli potato turn mash potato tablespoon mash potato per person mind tast good 00 per side dish short left hungri wonder spent entir meal next time we'll go well tri place steak durant' downtown phoenix worth drive\"\n",
      "0\t1.00\tb\"place suck realli deserv star like place know anyth vietnames food tri ... bun bo hue broth even tast like bun bo hue spici flavor where' beef shank hog' feet can't bun bo hue without shank hog' feet use cheapest cut meat item daili special pho chin nam broth super sweet know mean ... msg galor even hint anis star broth special pho can't even make broth tast right servic stink hate waiter hold bowl way thumb insid bowl serv dude never refil water ask thing etc veget platter fresh bean sprout start brown can't cook someth even bother serv custom know entre offer come back find one wors vietnames restaur valley check extens list vietnames restaur suggest better one\"\n"
     ]
    }
   ],
   "source": [
    "print('Truth Predicted Review')\n",
    "for x, y in zip(test_x, test_y):\n",
    "    y_hat = naive_bayes_predict(x, logprior, loglikelihood)\n",
    "    if y != (np.sign(y_hat) > 0):\n",
    "        print('%d\\t%0.2f\\t%s' % (y, np.sign(y_hat) > 0, ' '.join(\n",
    "            process_tweet(x)).encode('ascii', 'ignore')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Predict your own review__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.770884928244627\n"
     ]
    }
   ],
   "source": [
    "my_review = 'I am happy I visited this place, their burger is delicious :)'\n",
    "\n",
    "p = naive_bayes_predict(my_review, logprior, loglikelihood)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
